{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CorrelatedCounts Getting Started In order to run the correlated count models, you will need to install Python to your computer. It is most helpful to use miniconda , and then create an environment that will be used specifically with the correlated count package. The dependencies for this package are numpy and scipy . Installation To install the package, clone the GitHub repository here: git clone https : // github . com / mbannick / CorrelatedCounts . git Once you have cloned the package, activate your conda envirionment and run cd CorrelatedCounts python setup . py install The package name is ccount . Recommended Usage We recommend using Jupyter Notebooks (documentation here ) in your environment, which you can install with pip install jupyter and then running the command jupyter - notebook Once inside the notebook, you can import all of the functions that are described in the code documentation , including model specification , fitting , and prediction .","title":"Home"},{"location":"#welcome-to-correlatedcounts","text":"","title":"Welcome to CorrelatedCounts"},{"location":"#getting-started","text":"In order to run the correlated count models, you will need to install Python to your computer. It is most helpful to use miniconda , and then create an environment that will be used specifically with the correlated count package. The dependencies for this package are numpy and scipy .","title":"Getting Started"},{"location":"#installation","text":"To install the package, clone the GitHub repository here: git clone https : // github . com / mbannick / CorrelatedCounts . git Once you have cloned the package, activate your conda envirionment and run cd CorrelatedCounts python setup . py install The package name is ccount .","title":"Installation"},{"location":"#recommended-usage","text":"We recommend using Jupyter Notebooks (documentation here ) in your environment, which you can install with pip install jupyter and then running the command jupyter - notebook Once inside the notebook, you can import all of the functions that are described in the code documentation , including model specification , fitting , and prediction .","title":"Recommended Usage"},{"location":"code/","text":"Running a Model The core model is an object called ccount.core.CorrelatedModel . Each of the types of models you can fit are explained in the models documetation. Specifying the Model To specify a model, make sure that all variables that you need are contained within one data frame, including covariates, random effects, offsets, and weights. The outcome variables should be wide, but all other variables should be long, e.g. have one column for deaths and another column for cases, rather than one column with deaths and cases stacked on top of each other. All of these variables need to be filled in for every entry (i.e. there can be no missing values -- if you want to predict for somewhere with missing outcome information, you can do that later with the function that makes predictions). The function that you can use to initialize a model is ccount.run.convert_df_to_model . This function returns a CorrelatedModel object. It takes the following arguments: model_type : (str) The name of the model type to be fit. The list of names are in models . df : (pd.DataFrame) A data frame that contains all of your variables. outcome_variables : (List[str]) A list of the names of the outcome variables. fixed_effects : (List[List[List[str] or None]]) Nested lists of the names of fixed effects to put on each of the parameters and outcomes. See here for details. If you do not want to put any covariates on a parameter-outcome combination, pass None instead of List[str] . Note: Intercepts are automatically added for each parameter-outcome. Do not add another intercept. random_effect : (str) Name of the variable that specifies the grouping of the random effect. offset : List[str] (optional) List of variable names to use as an offset for each parameter. Must be of the length of the number of parameters in each model, and in the correct order . See here for the number and order of parameters for each model type. To include an offset on only one parameter, pass a list of the variable name and None , in the correct order corresponding to your model type. weight : (str) (optional) Name of the variable that specifies the weight to place on each observation. **kwargs : Additional arguments normalize_X : (bool) Whether or not to scale the covariates by their mean and standard deviation. By default, normalize_X = True . The resulting parameters are transformed after fitting so that they can be interpreted in the original space as the covariates. add_intercepts : (bool) Whether or not to add intercepts for all parameter-outcomes. By default, add_intercepts = True . Example Specification Consider a pandas data frame, df , that has the following columns: deaths , cases , median_income , population , country , age . In my example below, I will fit a Zero-Inflated Poisson model (described here ) with the following: Outcomes predict both deaths due to and cases of a disease, where deaths and cases are correlated with one another (2-dimensional outcome) Fixed Effects median income as a predictor for the mean of deaths age as a predictor for the mean of cases no predictors for the probability of a structural zero Random Effect random effect grouping by country, so that all data points within the same country will have the same 2-dimensional realization that correlates their deaths with their cases Offset population as the \"offset\" for the mean, or the denominator that deaths and cases came from from ccount.run import convert_df_to_model model = convert_df_to_model ( model_type = 'zero_inflated_poisson' , df = df , outcome_variables = [ 'deaths' , 'cases' ], fixed_effects = [[ None , None ], [[ 'median_income' ], [ 'age' ]]], random_effect = 'country' , offset = [ None , 'population' ] ) Fitting the Model Once you have the model object, returned by the function model = convert_df_to_model(...) , we can estimate the parameters. The class method ccount.core.CorrelatedModel.optimize_params does the optimization work. From our example above, to fit the model, we simply run: model . optimize_params ( max_iters = 10 ) You may pass in any integer for max_iters . A sensible range for the optimization routine implemented in this package is between 5-10 iterations. The parameter estimates, including the \\beta fixed effects, the U random effects, and the correlation between the outcomes given by D (each described in methods ) are all available in the summarize class method for ccount.core.CorrelatedModel . In our example above, to get a printed summary of the estimates (both transformed and un-transformed based on the link functions described in the model choices ), run the following: model . summarize () If you want to save the model summary to a file so that you can access it later, pass file=... and it will re-route the output to whatever file name (must have a .txt extension) that you pass. Creating Predictions In order to get the fitted values of deaths and cases, you can use the function ccount.run.get_predictions_from_df . It takes the same arguments as convert_df_to_model , except that in place of model_type , it needs a ccount.core.CorrelatedModel object, and it does not need the outcome variables. For our example above, this looks like from ccount.run import get_predictions_from_df predictions = get_predictions_from_df ( model = model , df = some_df , fixed_effects = [[ None , None ], [[ 'median_income' ], [ 'age' ]]], random_effect = 'country' , offset = [ None , 'population' ] ) predictions is a 2-dimensional array that is the length of the data frame some_df along one axis and the outcome predictions along the other (i.e. predictions for deaths and cases). The fixed_effects , random_effect , offset (if applicable), and weight (if applicable) arguments must be identical to those used in the initial model specification . Note that the data frame passed to get_predictions_from_df need not be the data frame that was used in model fitting. It can be a new data frame with missing outcome variables because they are not used in making predictions since the model has already been fit. However, this new data frame cannot have any missing values for random effects, fixed effects, or offsets. If it includes random effect grouping levels that were not observed in the fitting of the model, the random effect will be 0 for that level.","title":"Code"},{"location":"code/#running-a-model","text":"The core model is an object called ccount.core.CorrelatedModel . Each of the types of models you can fit are explained in the models documetation.","title":"Running a Model"},{"location":"code/#specifying-the-model","text":"To specify a model, make sure that all variables that you need are contained within one data frame, including covariates, random effects, offsets, and weights. The outcome variables should be wide, but all other variables should be long, e.g. have one column for deaths and another column for cases, rather than one column with deaths and cases stacked on top of each other. All of these variables need to be filled in for every entry (i.e. there can be no missing values -- if you want to predict for somewhere with missing outcome information, you can do that later with the function that makes predictions). The function that you can use to initialize a model is ccount.run.convert_df_to_model . This function returns a CorrelatedModel object. It takes the following arguments: model_type : (str) The name of the model type to be fit. The list of names are in models . df : (pd.DataFrame) A data frame that contains all of your variables. outcome_variables : (List[str]) A list of the names of the outcome variables. fixed_effects : (List[List[List[str] or None]]) Nested lists of the names of fixed effects to put on each of the parameters and outcomes. See here for details. If you do not want to put any covariates on a parameter-outcome combination, pass None instead of List[str] . Note: Intercepts are automatically added for each parameter-outcome. Do not add another intercept. random_effect : (str) Name of the variable that specifies the grouping of the random effect. offset : List[str] (optional) List of variable names to use as an offset for each parameter. Must be of the length of the number of parameters in each model, and in the correct order . See here for the number and order of parameters for each model type. To include an offset on only one parameter, pass a list of the variable name and None , in the correct order corresponding to your model type. weight : (str) (optional) Name of the variable that specifies the weight to place on each observation. **kwargs : Additional arguments normalize_X : (bool) Whether or not to scale the covariates by their mean and standard deviation. By default, normalize_X = True . The resulting parameters are transformed after fitting so that they can be interpreted in the original space as the covariates. add_intercepts : (bool) Whether or not to add intercepts for all parameter-outcomes. By default, add_intercepts = True .","title":"Specifying the Model"},{"location":"code/#example-specification","text":"Consider a pandas data frame, df , that has the following columns: deaths , cases , median_income , population , country , age . In my example below, I will fit a Zero-Inflated Poisson model (described here ) with the following: Outcomes predict both deaths due to and cases of a disease, where deaths and cases are correlated with one another (2-dimensional outcome) Fixed Effects median income as a predictor for the mean of deaths age as a predictor for the mean of cases no predictors for the probability of a structural zero Random Effect random effect grouping by country, so that all data points within the same country will have the same 2-dimensional realization that correlates their deaths with their cases Offset population as the \"offset\" for the mean, or the denominator that deaths and cases came from from ccount.run import convert_df_to_model model = convert_df_to_model ( model_type = 'zero_inflated_poisson' , df = df , outcome_variables = [ 'deaths' , 'cases' ], fixed_effects = [[ None , None ], [[ 'median_income' ], [ 'age' ]]], random_effect = 'country' , offset = [ None , 'population' ] )","title":"Example Specification"},{"location":"code/#fitting-the-model","text":"Once you have the model object, returned by the function model = convert_df_to_model(...) , we can estimate the parameters. The class method ccount.core.CorrelatedModel.optimize_params does the optimization work. From our example above, to fit the model, we simply run: model . optimize_params ( max_iters = 10 ) You may pass in any integer for max_iters . A sensible range for the optimization routine implemented in this package is between 5-10 iterations. The parameter estimates, including the \\beta fixed effects, the U random effects, and the correlation between the outcomes given by D (each described in methods ) are all available in the summarize class method for ccount.core.CorrelatedModel . In our example above, to get a printed summary of the estimates (both transformed and un-transformed based on the link functions described in the model choices ), run the following: model . summarize () If you want to save the model summary to a file so that you can access it later, pass file=... and it will re-route the output to whatever file name (must have a .txt extension) that you pass.","title":"Fitting the Model"},{"location":"code/#creating-predictions","text":"In order to get the fitted values of deaths and cases, you can use the function ccount.run.get_predictions_from_df . It takes the same arguments as convert_df_to_model , except that in place of model_type , it needs a ccount.core.CorrelatedModel object, and it does not need the outcome variables. For our example above, this looks like from ccount.run import get_predictions_from_df predictions = get_predictions_from_df ( model = model , df = some_df , fixed_effects = [[ None , None ], [[ 'median_income' ], [ 'age' ]]], random_effect = 'country' , offset = [ None , 'population' ] ) predictions is a 2-dimensional array that is the length of the data frame some_df along one axis and the outcome predictions along the other (i.e. predictions for deaths and cases). The fixed_effects , random_effect , offset (if applicable), and weight (if applicable) arguments must be identical to those used in the initial model specification . Note that the data frame passed to get_predictions_from_df need not be the data frame that was used in model fitting. It can be a new data frame with missing outcome variables because they are not used in making predictions since the model has already been fit. However, this new data frame cannot have any missing values for random effects, fixed effects, or offsets. If it includes random effect grouping levels that were not observed in the fitting of the model, the random effect will be 0 for that level.","title":"Creating Predictions"},{"location":"methods/","text":"Methods Statistical Model The correlated counts framework is useful for modeling the occurrences of multiple outcomes that arise from the same individual or observational unit (e.g. location). Consider m observations, with n outcomes. For example, one might model the number of faculty (outcome n=1) and students (outcome n=2) at 100 universities. We model these in a correlated framework because it is reasonable to assume that the number of faculty at a university is likely correlated with the number of students. Returning to the general case with m observations and n outcomes, in the linear context, we assume that the mean of the outcome is a function of the covariates for this outcome X_{i,j} , the coefficients to be estimated for this outcome \\beta{j} , and a random effect U_{i,j} E[Y_{i,j}|X_{i,j}, \\beta, U_{i,j}] = X_{i,j} \\beta_{j} + U_{i,j} for the i^{th} observation and the j^{th} outcome, where \\epsilon_{i} \\sim N(0, \\sigma^2) but with the additional assumption that the U_{i,} are multivariate normal, with mean 0 and covariance given by D . U_{i,.} \\sim N_{n}(0, D) \\quad D \\in \\mathbb{R}^n The U_{i,j} 's are correlated with one another, and that correlation drives correlation in the mean. This method follows that outlined by Rodrigues-Motta and colleages (2012) . In this simple case, Y \\sim Normal , but when working with counts, it is more common to use discrete distributions like the Poisson distribution or the Negative Binomial distribution. In cases where we have extremely rare events, we may also consider extensions to these distributions that allow for more zeros than would be typically realized in the discrete distributions (e.g. zero-inflation, hurdle models). As such, we will usually have more than one parameter to estimate besides the mean outcome. Most generally, consider that now we have l parameters. We have some probability distribution for Y , that is dependent on X_{i,j,k} , \\beta_{j,k} and U_{i,j,k} , f(Y_{i,j}|X_{i,j,k}, \\beta_{j,k}, U_{i,j,k} \\quad k = 1, ..., l) where the covariates X and random effects U can differ with respect to each of the l parameters of the discrete distribution (e.g. the mean and the over-dispersion parameter for the variance in the Negative Binomial distribution). Depending on the support for the parameter, we may need to transform the linear combination of X_{i,j,k} \\beta_{j,k} + U_{i,j,k} into the space that makes sense for the parameter and distribution at hand. For example, the mean of the Poisson distribution must be >0 , so a natural link function that does this transformation is e^{X_{i,j,k} \\beta_{j,k} + U_{i,j,k}} . Offsets Count data frequently arises from populations of varying sizes. For example, a count of deaths of 10 in a population of 100 represents a higher death rate than a 10 deaths in a population of 10,000. Therefore, when you have a model that you have parametrized such that some parameter affects the mean number of deaths, you will want to offset that mean by the population size. Adding an offset for the mean model means that you are effectively modeling the rate per offset unit, rather than the count alone. It is not advised to add an offset to parameters other than the mean. Likelihood Weights You may want to weight data points differently depending on how certain those data points are relative to others. This is implemented with a weighted likelihood approach: the likelihood contribution for each data point is multiplied by its corresponding weight (if weights are supplied). This will up-weight the contribution of more certain data points to the estimation problem, and down-weight the contribution of more uncertain data points. Optimization To estimate the parameters of this model, we add a prior for U that incorporates D , and then optimize f(Y_{i,j}|...) with respect to \\beta , U and D .","title":"Methods"},{"location":"methods/#methods","text":"","title":"Methods"},{"location":"methods/#statistical-model","text":"The correlated counts framework is useful for modeling the occurrences of multiple outcomes that arise from the same individual or observational unit (e.g. location). Consider m observations, with n outcomes. For example, one might model the number of faculty (outcome n=1) and students (outcome n=2) at 100 universities. We model these in a correlated framework because it is reasonable to assume that the number of faculty at a university is likely correlated with the number of students. Returning to the general case with m observations and n outcomes, in the linear context, we assume that the mean of the outcome is a function of the covariates for this outcome X_{i,j} , the coefficients to be estimated for this outcome \\beta{j} , and a random effect U_{i,j} E[Y_{i,j}|X_{i,j}, \\beta, U_{i,j}] = X_{i,j} \\beta_{j} + U_{i,j} for the i^{th} observation and the j^{th} outcome, where \\epsilon_{i} \\sim N(0, \\sigma^2) but with the additional assumption that the U_{i,} are multivariate normal, with mean 0 and covariance given by D . U_{i,.} \\sim N_{n}(0, D) \\quad D \\in \\mathbb{R}^n The U_{i,j} 's are correlated with one another, and that correlation drives correlation in the mean. This method follows that outlined by Rodrigues-Motta and colleages (2012) . In this simple case, Y \\sim Normal , but when working with counts, it is more common to use discrete distributions like the Poisson distribution or the Negative Binomial distribution. In cases where we have extremely rare events, we may also consider extensions to these distributions that allow for more zeros than would be typically realized in the discrete distributions (e.g. zero-inflation, hurdle models). As such, we will usually have more than one parameter to estimate besides the mean outcome. Most generally, consider that now we have l parameters. We have some probability distribution for Y , that is dependent on X_{i,j,k} , \\beta_{j,k} and U_{i,j,k} , f(Y_{i,j}|X_{i,j,k}, \\beta_{j,k}, U_{i,j,k} \\quad k = 1, ..., l) where the covariates X and random effects U can differ with respect to each of the l parameters of the discrete distribution (e.g. the mean and the over-dispersion parameter for the variance in the Negative Binomial distribution). Depending on the support for the parameter, we may need to transform the linear combination of X_{i,j,k} \\beta_{j,k} + U_{i,j,k} into the space that makes sense for the parameter and distribution at hand. For example, the mean of the Poisson distribution must be >0 , so a natural link function that does this transformation is e^{X_{i,j,k} \\beta_{j,k} + U_{i,j,k}} .","title":"Statistical Model"},{"location":"methods/#offsets","text":"Count data frequently arises from populations of varying sizes. For example, a count of deaths of 10 in a population of 100 represents a higher death rate than a 10 deaths in a population of 10,000. Therefore, when you have a model that you have parametrized such that some parameter affects the mean number of deaths, you will want to offset that mean by the population size. Adding an offset for the mean model means that you are effectively modeling the rate per offset unit, rather than the count alone. It is not advised to add an offset to parameters other than the mean.","title":"Offsets"},{"location":"methods/#likelihood-weights","text":"You may want to weight data points differently depending on how certain those data points are relative to others. This is implemented with a weighted likelihood approach: the likelihood contribution for each data point is multiplied by its corresponding weight (if weights are supplied). This will up-weight the contribution of more certain data points to the estimation problem, and down-weight the contribution of more uncertain data points.","title":"Likelihood Weights"},{"location":"methods/#optimization","text":"To estimate the parameters of this model, we add a prior for U that incorporates D , and then optimize f(Y_{i,j}|...) with respect to \\beta , U and D .","title":"Optimization"},{"location":"models/","text":"Available Models Parametrizing a Model Each of the available models is described below. They have different functional forms and therefore different parameters, and possibly different link functions for those parameters. The function to run a model is ccount.run.df_to_model , and requires the argument model_type . You determine the order of the outcomes by the order of the elements of the list of variable names that you pass as the outcomes argument, but you do not determine the order of the parameters in the model (that is explained below). The fixed effects \\beta are allowed to differ across the n outcomes and l parameters. The order in which you indicate the fixed_effects argument matters, and that order is described below. For example, for n=2 and l=2 , fixed_effects = [ [ [ 'covariate_1' ], # first parameter - first outcome [ 'covariate_1' ] # first parameter - second outcome ], [ [ 'covariate_1' , 'covariate_2' ], # second parameter - first outcome [ 'covariate_2' ] # second parameter - second outcome ] ] will add 'covariate_1' as a covariate for the first parameter on both outcomes, add 'covariate_2' as the only covariate for the second parameter and second outcome, and add both 'covariate_1' and 'covariate_2' as covariates for the second parameter and first outcome. Model Choices Zero-Inflated Poisson Model The Zero-Inflated Poisson model (ZIP) has a Poisson distribution, with a binomial distribution that determines that probability of the Poisson random variable realization being masked by an additional zero. Zeros can arise from the Poisson distribution or the Binomial distribution. Parameter 1 : probability of a structural zero (coming from the Binomial distribution) Parameter 2 : mean of the Poisson distribution To fit this model, use model_type = \"zero_inflated_poisson\" , which will use an exponential link function for the mean and the inverse logit function for the probability of a structural zero. Alternatively, you can use model_type = \"zero_inflated_poisson_relu\" , which will use a modified link function for the mean that is more stable for large values. Poisson Hurdle Model The Poisson hurdle model has a Poisson distribution that has been truncated at 0, and re-normalized for its new support, and a binomial model for the probability of a 0. In this model, zeros can only arise from the Binomial distribution. Parameter 1 : probability of a zero Parameter 2 : mean of the Poisson before truncation To fit this model, use model_type = \"hurdle_poisson\" , which will use an exponential link function for the mean and the inverse logit function for the probability of a structural zero. Alternatively, you can use model_type = \"hurdle_poisson_relu\" , which will use a modified link function for the mean that is more stable for large values. Negative Binomial The Negative Binomial model is an extension of the Poisson model that allows for over-dispersion (since in the Poisson distribution the mean = variance). Parameter 1 : mean of the distribution Parameter 2 : over-dispersion parameter (large values means variance >> mean) To fit this model, use model_type = \"negative_binomial\" , which will use an exponential link function for the mean and the over-dispersion parameter.","title":"Models"},{"location":"models/#available-models","text":"","title":"Available Models"},{"location":"models/#parametrizing-a-model","text":"Each of the available models is described below. They have different functional forms and therefore different parameters, and possibly different link functions for those parameters. The function to run a model is ccount.run.df_to_model , and requires the argument model_type . You determine the order of the outcomes by the order of the elements of the list of variable names that you pass as the outcomes argument, but you do not determine the order of the parameters in the model (that is explained below). The fixed effects \\beta are allowed to differ across the n outcomes and l parameters. The order in which you indicate the fixed_effects argument matters, and that order is described below. For example, for n=2 and l=2 , fixed_effects = [ [ [ 'covariate_1' ], # first parameter - first outcome [ 'covariate_1' ] # first parameter - second outcome ], [ [ 'covariate_1' , 'covariate_2' ], # second parameter - first outcome [ 'covariate_2' ] # second parameter - second outcome ] ] will add 'covariate_1' as a covariate for the first parameter on both outcomes, add 'covariate_2' as the only covariate for the second parameter and second outcome, and add both 'covariate_1' and 'covariate_2' as covariates for the second parameter and first outcome.","title":"Parametrizing a Model"},{"location":"models/#model-choices","text":"","title":"Model Choices"},{"location":"models/#zero-inflated-poisson-model","text":"The Zero-Inflated Poisson model (ZIP) has a Poisson distribution, with a binomial distribution that determines that probability of the Poisson random variable realization being masked by an additional zero. Zeros can arise from the Poisson distribution or the Binomial distribution. Parameter 1 : probability of a structural zero (coming from the Binomial distribution) Parameter 2 : mean of the Poisson distribution To fit this model, use model_type = \"zero_inflated_poisson\" , which will use an exponential link function for the mean and the inverse logit function for the probability of a structural zero. Alternatively, you can use model_type = \"zero_inflated_poisson_relu\" , which will use a modified link function for the mean that is more stable for large values.","title":"Zero-Inflated Poisson Model"},{"location":"models/#poisson-hurdle-model","text":"The Poisson hurdle model has a Poisson distribution that has been truncated at 0, and re-normalized for its new support, and a binomial model for the probability of a 0. In this model, zeros can only arise from the Binomial distribution. Parameter 1 : probability of a zero Parameter 2 : mean of the Poisson before truncation To fit this model, use model_type = \"hurdle_poisson\" , which will use an exponential link function for the mean and the inverse logit function for the probability of a structural zero. Alternatively, you can use model_type = \"hurdle_poisson_relu\" , which will use a modified link function for the mean that is more stable for large values.","title":"Poisson Hurdle Model"},{"location":"models/#negative-binomial","text":"The Negative Binomial model is an extension of the Poisson model that allows for over-dispersion (since in the Poisson distribution the mean = variance). Parameter 1 : mean of the distribution Parameter 2 : over-dispersion parameter (large values means variance >> mean) To fit this model, use model_type = \"negative_binomial\" , which will use an exponential link function for the mean and the over-dispersion parameter.","title":"Negative Binomial"}]}